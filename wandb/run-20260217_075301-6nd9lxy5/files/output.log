   GPU: NVIDIA GeForce RTX 3050 Ti Laptop GPU
   CUDA Version: 12.1

============================================================
STARTING RANDOM EPISODE COLLECTION
Target episodes: 5
Max steps per episode: 1000
============================================================

 Episode 1/5 - Starting...
  Resetting environment...
    Step 100/1000 | Reward: 4.49
    Step 200/1000 | Reward: 7.52
    Step 300/1000 | Reward: 10.46
    Step 400/1000 | Reward: 13.01
    Step 500/1000 | Reward: 17.87
    Step 600/1000 | Reward: 22.58
    Step 700/1000 | Reward: 27.19
    Step 800/1000 | Reward: 29.55
    Step 900/1000 | Reward: 32.96
    Step 1000/1000 | Reward: 40.57
  Episode ended at step 1000 (terminated)
     Episode 1 complete!
     Steps: 1000 | Reward: 40.570
     Running averages - Steps: 1000.0 | Reward: 40.570

 Episode 2/5 - Starting...
  Resetting environment...
    Step 100/1000 | Reward: 3.78
    Step 200/1000 | Reward: 6.66
    Step 300/1000 | Reward: 9.85
    Step 400/1000 | Reward: 12.61
    Step 500/1000 | Reward: 17.33
    Step 600/1000 | Reward: 20.10
    Step 700/1000 | Reward: 23.15
    Step 800/1000 | Reward: 26.35
    Step 900/1000 | Reward: 28.85
    Step 1000/1000 | Reward: 32.27
  Episode ended at step 1000 (terminated)
     Episode 2 complete!
     Steps: 1000 | Reward: 32.273
     Running averages - Steps: 1000.0 | Reward: 36.421

 Episode 3/5 - Starting...
  Resetting environment...
    Step 100/1000 | Reward: 5.01
    Step 200/1000 | Reward: 7.47
    Step 300/1000 | Reward: 10.48
    Step 400/1000 | Reward: 13.11
    Step 500/1000 | Reward: 15.92
    Step 600/1000 | Reward: 18.51
    Step 700/1000 | Reward: 20.79
    Step 800/1000 | Reward: 23.15
    Step 900/1000 | Reward: 25.31
    Step 1000/1000 | Reward: 28.21
  Episode ended at step 1000 (terminated)
     Episode 3 complete!
     Steps: 1000 | Reward: 28.213
     Running averages - Steps: 1000.0 | Reward: 33.685

 Episode 4/5 - Starting...
  Resetting environment...
    Step 100/1000 | Reward: 3.80
    Step 200/1000 | Reward: 6.51
    Step 300/1000 | Reward: 9.27
    Step 400/1000 | Reward: 13.19
    Step 500/1000 | Reward: 16.40
    Step 600/1000 | Reward: 19.07
    Step 700/1000 | Reward: 21.55
    Step 800/1000 | Reward: 24.04
    Step 900/1000 | Reward: 26.54
    Step 1000/1000 | Reward: 29.45
  Episode ended at step 1000 (terminated)
     Episode 4 complete!
     Steps: 1000 | Reward: 29.454
     Running averages - Steps: 1000.0 | Reward: 32.627

 Episode 5/5 - Starting...
  Resetting environment...
    Step 100/1000 | Reward: 3.38
    Step 200/1000 | Reward: 6.20
    Step 300/1000 | Reward: 8.97
    Step 400/1000 | Reward: 11.82
    Step 500/1000 | Reward: 14.73
    Step 600/1000 | Reward: 18.04
    Step 700/1000 | Reward: 22.62
    Step 800/1000 | Reward: 25.42
    Step 900/1000 | Reward: 28.92
    Step 1000/1000 | Reward: 32.24
  Episode ended at step 1000 (terminated)
     Episode 5 complete!
     Steps: 1000 | Reward: 32.241
     Running averages - Steps: 1000.0 | Reward: 32.550

 PROGRESS UPDATE:
   Episodes completed: 5/5
   Total timesteps collected: 5000
   Average episode length: 1000.0 steps
   Average episode reward: 32.550

============================================================
RANDOM EPISODE COLLECTION COMPLETE!
Total episodes: 5
Total timesteps: 5000
Average episode length: 1000.0 steps
Average episode reward: 32.550
============================================================

 Starting epoch 0/100000...
   Current dataset size: 5000 timesteps
   Batch shapes - Obs: torch.Size([50, 50, 3, 64, 64]), Actions: torch.Size([50, 50, 6]), Rewards: torch.Size([50, 50])
   Processing batch: 50 sequences x 50 timesteps
   Observation shape: 3x64x64
        DECODE DEBUG - Hidden: torch.Size([50, 200]), Posterior: torch.Size([50, 30])
        DECODE DEBUG - Combined input: torch.Size([50, 230])
        DECODE DEBUG - Hidden: torch.Size([50, 200]), Posterior: torch.Size([50, 30])
        DECODE DEBUG - Combined input: torch.Size([50, 230])
        DECODE DEBUG - Hidden: torch.Size([50, 200]), Posterior: torch.Size([50, 30])
        DECODE DEBUG - Combined input: torch.Size([50, 230])
    DEBUG - Recon Loss (Summed): 909.19
    DEBUG - Reward Loss: 0.92
    DEBUG - Raw KL: 0.14
Traceback (most recent call last):
  File "/home/patrick/Documents/Dreamer/training.py", line 661, in <module>
    trained_rssm = train_rssm(
  File "/home/patrick/Documents/Dreamer/training.py", line 545, in train_rssm
    state_values, rewards, states, imagined_hiddens, actions = imagine_trajectories(rssm, action_model, value_model, posterior_states.detach(), hiddens.detach(), lmbda = 0.95, discount = 0.99, horizon = 15)
  File "/home/patrick/Documents/Dreamer/training.py", line 215, in imagine_trajectories
    prev_state, prev_hidden, reward = rssm.imagine_one_step(prev_state, prev_hidden, action)
  File "/home/patrick/Documents/Dreamer/models/RSSM.py", line 178, in imagine_one_step
    new_hidden = self.rnn(embedded, prev_hidden) # predict the next hidden to predict the next latent state from
  File "/home/patrick/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patrick/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patrick/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 1794, in forward
    raise ValueError(
ValueError: GRUCell: Expected input to be 1D or 2D, got 3D instead
Traceback (most recent call last):
  File "/home/patrick/Documents/Dreamer/training.py", line 661, in <module>
    trained_rssm = train_rssm(
  File "/home/patrick/Documents/Dreamer/training.py", line 545, in train_rssm
    state_values, rewards, states, imagined_hiddens, actions = imagine_trajectories(rssm, action_model, value_model, posterior_states.detach(), hiddens.detach(), lmbda = 0.95, discount = 0.99, horizon = 15)
  File "/home/patrick/Documents/Dreamer/training.py", line 215, in imagine_trajectories
    prev_state, prev_hidden, reward = rssm.imagine_one_step(prev_state, prev_hidden, action)
  File "/home/patrick/Documents/Dreamer/models/RSSM.py", line 178, in imagine_one_step
    new_hidden = self.rnn(embedded, prev_hidden) # predict the next hidden to predict the next latent state from
  File "/home/patrick/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/patrick/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/patrick/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py", line 1794, in forward
    raise ValueError(
ValueError: GRUCell: Expected input to be 1D or 2D, got 3D instead
